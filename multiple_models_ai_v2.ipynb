{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSFwTkeGCGsaPjL7cjrR4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/100profes/blob/master/multiple_models_ai_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de m√∫ltiples modelos de AI\n"
      ],
      "metadata": {
        "id": "37UEQHo0U3Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab Notebook: Integrating Mistral with Hugging Face Narrow AI Models\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers requests torch datasets diffusers\n",
        "\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Step 1: Request API Key from User\n",
        "mistral_api_key = input(\"Please enter your Mistral API Key: \")\n",
        "\n",
        "# Function to interact with Mistral\n",
        "def query_mistral(prompt, api_key):\n",
        "    url = \"https://api.mistral.ai/v1/chat\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    payload = {\"prompt\": prompt}\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"response\", \"No response from Mistral.\")\n",
        "    else:\n",
        "        return f\"Error: {response.status_code}, {response.text}\"\n",
        "\n",
        "# Step 2: Load Hugging Face Models\n",
        "print(\"Loading Hugging Face models...\")\n",
        "\n",
        "# Model 1: Text-to-Image\n",
        "try:\n",
        "    t2i_model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
        "    t2i_model = t2i_model.to(\"cuda\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading text-to-image model: {e}\")\n",
        "\n",
        "# Model 2: Text-to-Audio (example placeholder, replace with actual model)\n",
        "t2a_model = None  # Replace with valid Hugging Face text-to-audio model\n",
        "\n",
        "# Model 3: Text-to-Text\n",
        "t2t_model = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n",
        "# Step 3: Main Interaction Loop\n",
        "def main():\n",
        "    print(\"Chat with Mistral. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Query Mistral\n",
        "        mistral_response = query_mistral(user_input, mistral_api_key)\n",
        "        print(f\"Mistral: {mistral_response}\")\n",
        "\n",
        "        # Check if Mistral suggests using a specific HF model\n",
        "        if \"use text-to-image\" in mistral_response.lower():\n",
        "            image_description = input(\"Enter description for image generation: \")\n",
        "            try:\n",
        "                image = t2i_model(image_description).images[0]\n",
        "                image.show()  # Display image in Colab\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating image: {e}\")\n",
        "        elif \"use text-to-audio\" in mistral_response.lower():\n",
        "            print(\"Text-to-audio model is not configured.\")\n",
        "        elif \"use text-to-text\" in mistral_response.lower():\n",
        "            text_description = input(\"Enter text for transformation: \")\n",
        "            text_output = t2t_model(text_description)\n",
        "            print(\"Generated Text:\", text_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gkRIzzdGWbUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}