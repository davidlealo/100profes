{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC1OiGHcRT2WHOfg1/CcO1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/100profes/blob/master/multiple_models_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de múltiples modelos de AI\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9itGieFvwJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 1: Instalación de librerías"
      ],
      "metadata": {
        "id": "UoGXd2qzwVoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDA5xmtDvsle",
        "outputId": "ab59ea89-39f6-4734-8ac0-c7693126cb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio requests -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 2: Configuración inicial"
      ],
      "metadata": {
        "id": "Go7MiNi4wZ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Variable global para almacenar la API key del usuario\n",
        "user_api_key = None"
      ],
      "metadata": {
        "id": "PiSlzJiIwdrW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 3: Función para interactuar con Mistral\n"
      ],
      "metadata": {
        "id": "zmBL_cCewhfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_request(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Envía un prompt a la API de Mistral y devuelve la respuesta.\n",
        "    \"\"\"\n",
        "    url = \"https://api.mistral.ai/v1/completions\"  # Cambia si usas otro endpoint\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"mistral\",  # O el modelo que uses\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"choices\")[0].get(\"text\", \"No response.\")\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\""
      ],
      "metadata": {
        "id": "TWLWvdG2wiwV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 4: Identificar intención y asignar acción\n"
      ],
      "metadata": {
        "id": "fyt3zFGUw0Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_and_call_model(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Detecta la intención del usuario y realiza la acción correspondiente.\n",
        "    \"\"\"\n",
        "    if \"análisis de sentimientos\" in prompt.lower():\n",
        "        return mistral_request(\"Realiza un análisis de sentimientos: \" + prompt, api_key)\n",
        "    elif \"traducción\" in prompt.lower():\n",
        "        return mistral_request(\"Traduce lo siguiente: \" + prompt, api_key)\n",
        "    else:\n",
        "        return mistral_request(prompt, api_key)\n"
      ],
      "metadata": {
        "id": "tu6nONBMw3jd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 5: Interfaz con Gradio"
      ],
      "metadata": {
        "id": "kBuLPiZmw4Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 5: Interfaz con Gradio (Separar configuración y chat)\n",
        "\n",
        "# Interfaz para configurar la API Key\n",
        "def api_key_interface(api_key):\n",
        "    \"\"\"\n",
        "    Guarda la API Key del usuario.\n",
        "    \"\"\"\n",
        "    global user_api_key\n",
        "    user_api_key = api_key\n",
        "    return \"API Key configurada correctamente.\"\n",
        "\n",
        "# Interfaz para el chat\n",
        "def chat_interface(user_input):\n",
        "    \"\"\"\n",
        "    Procesa el prompt del usuario y devuelve la respuesta del modelo.\n",
        "    \"\"\"\n",
        "    if not user_api_key:\n",
        "        return \"Por favor, configura tu API Key primero.\"\n",
        "    try:\n",
        "        response = identify_and_call_model(user_input, user_api_key)\n",
        "        return f\"Respuesta de la AI:\\n{response}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Crear dos interfaces independientes\n",
        "api_key_ui = gr.Interface(\n",
        "    fn=api_key_interface,\n",
        "    inputs=gr.Textbox(label=\"Introduce tu API Key\", type=\"password\", placeholder=\"Tu API Key aquí\"),\n",
        "    outputs=gr.Textbox(label=\"Estado de configuración\"),\n",
        "    title=\"Configurar API Key\",\n",
        "    description=\"Introduce tu API Key de Mistral para comenzar.\"\n",
        ")\n",
        "\n",
        "chat_ui = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Escribe tu solicitud aquí...\"),\n",
        "    outputs=gr.Textbox(label=\"Respuesta\", lines=10),\n",
        "    title=\"Chat con Mistral\",\n",
        "    description=\"Envía un prompt y obtén una respuesta del modelo.\"\n",
        ")\n",
        "\n",
        "# Combinar las interfaces en un espacio de trabajo\n",
        "gr.TabbedInterface(\n",
        "    [api_key_ui, chat_ui],\n",
        "    [\"Configurar API Key\", \"Chat\"]\n",
        ").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "G8dDr0NxwRSi",
        "outputId": "9e398971-874a-474e-a658-c53bbbca862d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://31c63d481e64d78b1b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://31c63d481e64d78b1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuevo enfoque"
      ],
      "metadata": {
        "id": "DoEFTzE3x3hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Modelos predefinidos con tareas compatibles\n",
        "models = {\n",
        "    \"text-to-image\": \"CompVis/stable-diffusion-v1-4\",  # Imagen desde texto\n",
        "    \"text-to-video\": \"runwayml/stable-diffusion-v1-5\",  # Generación de video\n",
        "    \"text-to-audio\": \"suno/bark\",  # Generación de audio\n",
        "    \"text-generation\": \"gpt2\"  # Generación de texto\n",
        "}\n",
        "\n",
        "# Función para cargar pipelines dinámicamente según el modelo\n",
        "pipelines = {}\n",
        "\n",
        "def load_model(task):\n",
        "    \"\"\"\n",
        "    Carga el pipeline de Hugging Face según la tarea.\n",
        "    \"\"\"\n",
        "    if task not in pipelines:\n",
        "        pipelines[task] = pipeline(task=task, model=models[task])\n",
        "    return pipelines[task]\n",
        "\n",
        "# Paso 3: Detección de intención\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta el tipo de tarea basado en palabras clave en el prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    elif any(keyword in prompt.lower() for keyword in [\"video\", \"animación\"]):\n",
        "        return \"text-to-video\"\n",
        "    elif any(keyword in prompt.lower() for keyword in [\"audio\", \"música\", \"sonido\"]):\n",
        "        return \"text-to-audio\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto: generación de texto\n",
        "\n",
        "# Paso 4: Procesar solicitud según el modelo\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la intención del usuario, carga el modelo correspondiente y genera un resultado.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Cargar el pipeline adecuado\n",
        "    model_pipeline = load_model(task)\n",
        "\n",
        "    # Generar salida\n",
        "    if task == \"text-to-image\":\n",
        "        # La salida puede ser una imagen (se ajusta según el pipeline)\n",
        "        return \"Imagen generada exitosamente (implementa visualización).\"\n",
        "    elif task == \"text-to-video\":\n",
        "        return \"Video generado exitosamente (implementa visualización).\"\n",
        "    elif task == \"text-to-audio\":\n",
        "        return \"Audio generado exitosamente (implementa visualización).\"\n",
        "    else:\n",
        "        # Para tareas de texto\n",
        "        result = model_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
        "        return f\"Task: {task}\\n\\nOutput:\\n{result[0]['generated_text']}\"\n",
        "\n",
        "# Paso 5: Interfaz con Gradio\n",
        "import gradio as gr\n",
        "\n",
        "# Interfaz única para procesar prompts\n",
        "def main_function(user_input):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y devuelve el resultado del modelo adecuado.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = process_prompt(user_input)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas (imagen, video, audio, texto...)\"),\n",
        "    outputs=gr.Textbox(label=\"Resultado\", lines=10),\n",
        "    title=\"Chat Multi-Modo con Modelos Específicos\",\n",
        "    description=\"Un chatbot que detecta automáticamente si necesitas texto, imagen, video o audio.\"\n",
        ")\n",
        "\n",
        "# Ejecutar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "doXdWkyHyTeU",
        "outputId": "c9aa85d0-e1e4-48a6-9861-6ce8f78feb3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ff9e625e23bd2f0317.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff9e625e23bd2f0317.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}