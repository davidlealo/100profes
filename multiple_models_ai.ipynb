{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj84cgbk9NBlAV/mswKRXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/100profes/blob/master/multiple_models_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de múltiples modelos de AI\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9itGieFvwJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 1: Instalación de librerías"
      ],
      "metadata": {
        "id": "UoGXd2qzwVoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDA5xmtDvsle",
        "outputId": "ab59ea89-39f6-4734-8ac0-c7693126cb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio requests -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 2: Configuración inicial"
      ],
      "metadata": {
        "id": "Go7MiNi4wZ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Variable global para almacenar la API key del usuario\n",
        "user_api_key = None"
      ],
      "metadata": {
        "id": "PiSlzJiIwdrW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 3: Función para interactuar con Mistral\n"
      ],
      "metadata": {
        "id": "zmBL_cCewhfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_request(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Envía un prompt a la API de Mistral y devuelve la respuesta.\n",
        "    \"\"\"\n",
        "    url = \"https://api.mistral.ai/v1/completions\"  # Cambia si usas otro endpoint\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"mistral\",  # O el modelo que uses\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"choices\")[0].get(\"text\", \"No response.\")\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\""
      ],
      "metadata": {
        "id": "TWLWvdG2wiwV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Paso 4: Identificar intención y asignar acción\n",
        "def identify_and_call_model(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Detecta la intención del usuario y realiza la acción correspondiente.\n",
        "    \"\"\"\n",
        "    if \"análisis de sentimientos\" in prompt.lower():\n",
        "        return mistral_request(\"Realiza un análisis de sentimientos: \" + prompt, api_key)\n",
        "    elif \"traducción\" in prompt.lower():\n",
        "        return mistral_request(\"Traduce lo siguiente: \" + prompt, api_key)\n",
        "    else:\n",
        "        return mistral_request(prompt, api_key)\n",
        "\n",
        "# Paso 5: Interfaz con Gradio\n",
        "import gradio as gr\n",
        "\n",
        "def set_api_key(api_key):\n",
        "    \"\"\"\n",
        "    Guarda la API key proporcionada por el usuario.\n",
        "    \"\"\"\n",
        "    global user_api_key\n",
        "    user_api_key = api_key\n",
        "    return \"API Key configurada correctamente.\"\n",
        "\n",
        "def main_function(user_input):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y llama al modelo adecuado.\n",
        "    \"\"\"\n",
        "    if not user_api_key:\n",
        "        return \"Por favor, configura tu API Key primero.\"\n",
        "\n",
        "    try:\n",
        "        response = identify_and_call_model(user_input, user_api_key)\n",
        "        return f\"Respuesta de la AI:\\n{response}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Crear la interfaz de Gradio\n",
        "api_key_input = gr.Textbox(label=\"Introduce tu API Key\", type=\"password\", placeholder=\"Tu API Key aquí\")\n",
        "set_key_button = gr.Button(\"Configurar API Key\")\n",
        "chat_input = gr.Textbox(label=\"Prompt\", placeholder=\"Escribe tu solicitud aquí...\")\n",
        "chat_output = gr.Textbox(label=\"Respuesta\", lines=10)\n",
        "\n",
        "# Interfaz con dos funciones: configurar API Key y procesar el chat\n",
        "interface = gr.Interface(\n",
        "    fn=[set_api_key, main_function],\n",
        "    inputs=[api_key_input, chat_input],\n",
        "    outputs=chat_output,\n",
        "    live=True,\n",
        "    title=\"Chat con Mistral\",\n",
        "    description=\"Un chatbot que entiende tus necesidades y usa modelos de Mistral.\"\n",
        ")\n",
        "\n",
        "# Configuración de botones\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "G8dDr0NxwRSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}