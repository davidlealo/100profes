{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGUJwuyE+duoztiqKNd69r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/100profes/blob/master/multiple_models_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de múltiples modelos de AI\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9itGieFvwJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 1: Instalación de librerías"
      ],
      "metadata": {
        "id": "UoGXd2qzwVoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDA5xmtDvsle",
        "outputId": "ab59ea89-39f6-4734-8ac0-c7693126cb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio requests -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 2: Configuración inicial"
      ],
      "metadata": {
        "id": "Go7MiNi4wZ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Variable global para almacenar la API key del usuario\n",
        "user_api_key = None"
      ],
      "metadata": {
        "id": "PiSlzJiIwdrW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 3: Función para interactuar con Mistral\n"
      ],
      "metadata": {
        "id": "zmBL_cCewhfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral_request(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Envía un prompt a la API de Mistral y devuelve la respuesta.\n",
        "    \"\"\"\n",
        "    url = \"https://api.mistral.ai/v1/completions\"  # Cambia si usas otro endpoint\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"mistral\",  # O el modelo que uses\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"choices\")[0].get(\"text\", \"No response.\")\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\""
      ],
      "metadata": {
        "id": "TWLWvdG2wiwV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 4: Identificar intención y asignar acción\n"
      ],
      "metadata": {
        "id": "fyt3zFGUw0Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_and_call_model(prompt, api_key):\n",
        "    \"\"\"\n",
        "    Detecta la intención del usuario y realiza la acción correspondiente.\n",
        "    \"\"\"\n",
        "    if \"análisis de sentimientos\" in prompt.lower():\n",
        "        return mistral_request(\"Realiza un análisis de sentimientos: \" + prompt, api_key)\n",
        "    elif \"traducción\" in prompt.lower():\n",
        "        return mistral_request(\"Traduce lo siguiente: \" + prompt, api_key)\n",
        "    else:\n",
        "        return mistral_request(prompt, api_key)\n"
      ],
      "metadata": {
        "id": "tu6nONBMw3jd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paso 5: Interfaz con Gradio"
      ],
      "metadata": {
        "id": "kBuLPiZmw4Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 5: Interfaz con Gradio (Separar configuración y chat)\n",
        "\n",
        "# Interfaz para configurar la API Key\n",
        "def api_key_interface(api_key):\n",
        "    \"\"\"\n",
        "    Guarda la API Key del usuario.\n",
        "    \"\"\"\n",
        "    global user_api_key\n",
        "    user_api_key = api_key\n",
        "    return \"API Key configurada correctamente.\"\n",
        "\n",
        "# Interfaz para el chat\n",
        "def chat_interface(user_input):\n",
        "    \"\"\"\n",
        "    Procesa el prompt del usuario y devuelve la respuesta del modelo.\n",
        "    \"\"\"\n",
        "    if not user_api_key:\n",
        "        return \"Por favor, configura tu API Key primero.\"\n",
        "    try:\n",
        "        response = identify_and_call_model(user_input, user_api_key)\n",
        "        return f\"Respuesta de la AI:\\n{response}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Crear dos interfaces independientes\n",
        "api_key_ui = gr.Interface(\n",
        "    fn=api_key_interface,\n",
        "    inputs=gr.Textbox(label=\"Introduce tu API Key\", type=\"password\", placeholder=\"Tu API Key aquí\"),\n",
        "    outputs=gr.Textbox(label=\"Estado de configuración\"),\n",
        "    title=\"Configurar API Key\",\n",
        "    description=\"Introduce tu API Key de Mistral para comenzar.\"\n",
        ")\n",
        "\n",
        "chat_ui = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Escribe tu solicitud aquí...\"),\n",
        "    outputs=gr.Textbox(label=\"Respuesta\", lines=10),\n",
        "    title=\"Chat con Mistral\",\n",
        "    description=\"Envía un prompt y obtén una respuesta del modelo.\"\n",
        ")\n",
        "\n",
        "# Combinar las interfaces en un espacio de trabajo\n",
        "gr.TabbedInterface(\n",
        "    [api_key_ui, chat_ui],\n",
        "    [\"Configurar API Key\", \"Chat\"]\n",
        ").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "G8dDr0NxwRSi",
        "outputId": "9e398971-874a-474e-a658-c53bbbca862d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://31c63d481e64d78b1b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://31c63d481e64d78b1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuevo enfoque"
      ],
      "metadata": {
        "id": "DoEFTzE3x3hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Modelos predefinidos con tareas compatibles\n",
        "models = {\n",
        "    \"text-to-image\": \"CompVis/stable-diffusion-v1-4\",  # Imagen desde texto\n",
        "    \"text-to-video\": \"runwayml/stable-diffusion-v1-5\",  # Generación de video\n",
        "    \"text-to-audio\": \"suno/bark\",  # Generación de audio\n",
        "    \"text-generation\": \"gpt2\"  # Generación de texto\n",
        "}\n",
        "\n",
        "# Función para cargar pipelines dinámicamente según el modelo\n",
        "pipelines = {}\n",
        "\n",
        "def load_model(task):\n",
        "    \"\"\"\n",
        "    Carga el pipeline de Hugging Face según la tarea.\n",
        "    \"\"\"\n",
        "    if task not in pipelines:\n",
        "        pipelines[task] = pipeline(task=task, model=models[task])\n",
        "    return pipelines[task]\n",
        "\n",
        "# Paso 3: Detección de intención\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta el tipo de tarea basado en palabras clave en el prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    elif any(keyword in prompt.lower() for keyword in [\"video\", \"animación\"]):\n",
        "        return \"text-to-video\"\n",
        "    elif any(keyword in prompt.lower() for keyword in [\"audio\", \"música\", \"sonido\"]):\n",
        "        return \"text-to-audio\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto: generación de texto\n",
        "\n",
        "# Paso 4: Procesar solicitud según el modelo\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la intención del usuario, carga el modelo correspondiente y genera un resultado.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Cargar el pipeline adecuado\n",
        "    model_pipeline = load_model(task)\n",
        "\n",
        "    # Generar salida\n",
        "    if task == \"text-to-image\":\n",
        "        # La salida puede ser una imagen (se ajusta según el pipeline)\n",
        "        return \"Imagen generada exitosamente (implementa visualización).\"\n",
        "    elif task == \"text-to-video\":\n",
        "        return \"Video generado exitosamente (implementa visualización).\"\n",
        "    elif task == \"text-to-audio\":\n",
        "        return \"Audio generado exitosamente (implementa visualización).\"\n",
        "    else:\n",
        "        # Para tareas de texto\n",
        "        result = model_pipeline(prompt, max_length=100, num_return_sequences=1)\n",
        "        return f\"Task: {task}\\n\\nOutput:\\n{result[0]['generated_text']}\"\n",
        "\n",
        "# Paso 5: Interfaz con Gradio\n",
        "import gradio as gr\n",
        "\n",
        "# Interfaz única para procesar prompts\n",
        "def main_function(user_input):\n",
        "    \"\"\"\n",
        "    Procesa la entrada del usuario y devuelve el resultado del modelo adecuado.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = process_prompt(user_input)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas (imagen, video, audio, texto...)\"),\n",
        "    outputs=gr.Textbox(label=\"Resultado\", lines=10),\n",
        "    title=\"Chat Multi-Modo con Modelos Específicos\",\n",
        "    description=\"Un chatbot que detecta automáticamente si necesitas texto, imagen, video o audio.\"\n",
        ")\n",
        "\n",
        "# Ejecutar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "doXdWkyHyTeU",
        "outputId": "c9aa85d0-e1e4-48a6-9861-6ce8f78feb3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ff9e625e23bd2f0317.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ff9e625e23bd2f0317.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enfoque imagenes"
      ],
      "metadata": {
        "id": "zw35dSqRyyOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando un modelo de Diffusers.\n",
        "    \"\"\"\n",
        "    model_id = models[\"text-to-image\"]\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    pipe = pipe.to(\"cuda\")  # Asegúrate de tener una GPU\n",
        "    image = pipe(prompt).images[0]\n",
        "    return image\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    model_id = models[\"text-generation\"]\n",
        "    text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "    output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "    return output[0][\"generated_text\"]\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    if task == \"text-to-image\":\n",
        "        return generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "import gradio as gr\n",
        "\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    try:\n",
        "        if task == \"text-to-image\":\n",
        "            return process_prompt(prompt)\n",
        "        elif task == \"text-generation\":\n",
        "            return process_prompt(prompt)\n",
        "    except Exception as e:\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Creación de interfaces específicas\n",
        "text_output = gr.Textbox(label=\"Resultado (Texto)\")\n",
        "image_output = gr.Image(label=\"Resultado (Imagen)\")\n",
        "\n",
        "# Interfaz principal con salidas dinámicas\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[image_output, text_output],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "F0XwfgKGyz6m",
        "outputId": "794b49d1-5e8c-47af-eb95-a0a8bc0b939d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRunning Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://381e78b615d9646dde.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://381e78b615d9646dde.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# otro enfoque"
      ],
      "metadata": {
        "id": "_GJ_Yjd1z8iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(\"cuda\")  # Asegúrate de usar GPU\n",
        "\n",
        "        # Generar imagen\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando la imagen: {str(e)}\")\n",
        "        return f\"Error generando la imagen: {str(e)}\"\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando el texto: {str(e)}\")\n",
        "        return f\"Error generando el texto: {str(e)}\"\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Liberar memoria antes de cargar modelos\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if task == \"text-to-image\":\n",
        "        return generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    try:\n",
        "        if task == \"text-to-image\":\n",
        "            return process_prompt(prompt)\n",
        "        elif task == \"text-generation\":\n",
        "            return process_prompt(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando tu solicitud: {str(e)}\")\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch(server_name=\"0.0.0.0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "cy8qRPQ_0Krc",
        "outputId": "6e6bec08-cebc-41fa-800f-b1362296402b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://544bf5eca5e834cf72.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://544bf5eca5e834cf72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# enfoque"
      ],
      "metadata": {
        "id": "EVHNEGpe1Qqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(\"cuda\")  # Asegúrate de usar GPU\n",
        "\n",
        "        # Generar imagen\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando la imagen: {str(e)}\")\n",
        "        return f\"Error generando la imagen: {str(e)}\"\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando el texto: {str(e)}\")\n",
        "        return f\"Error generando el texto: {str(e)}\"\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Liberar memoria antes de cargar modelos\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if task == \"text-to-image\":\n",
        "        return generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    try:\n",
        "        if task == \"text-to-image\":\n",
        "            return process_prompt(prompt)\n",
        "        elif task == \"text-generation\":\n",
        "            return process_prompt(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando tu solicitud: {str(e)}\")\n",
        "        return f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "xx-ucYhd1R4N",
        "outputId": "aaf0e8bd-415b-4876-e5c6-a021141d1dd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://620e2dd5abee23315a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://620e2dd5abee23315a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# enfoque"
      ],
      "metadata": {
        "id": "hy15GYwd1r-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verificar disponibilidad de GPU\n",
        "        if not torch.cuda.is_available():\n",
        "            return \"Error: GPU no disponible. Asegúrate de usar un entorno con GPU.\"\n",
        "\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        print(\"Cargando modelo de Diffusers para text-to-image...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(\"cuda\")\n",
        "\n",
        "        # Generar imagen\n",
        "        print(\"Generando imagen...\")\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando la imagen: {str(e)}\")\n",
        "        return f\"Error generando la imagen: {str(e)}\"\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        print(\"Cargando modelo de Transformers para text-generation...\")\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        print(\"Generando texto...\")\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando el texto: {str(e)}\")\n",
        "        return f\"Error generando el texto: {str(e)}\"\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Liberar memoria antes de cargar modelos\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if task == \"text-to-image\":\n",
        "        return generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    try:\n",
        "        print(f\"Detectando tarea: {task}\")\n",
        "        if task == \"text-to-image\":\n",
        "            return process_prompt(prompt), None\n",
        "        elif task == \"text-generation\":\n",
        "            return None, process_prompt(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando tu solicitud: {str(e)}\")\n",
        "        return None, f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "_imwWNrn1tJz",
        "outputId": "0909ff4e-b742-43f7-c68e-c56e59533b41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://52b162941962124446.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://52b162941962124446.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# enfoque"
      ],
      "metadata": {
        "id": "9XKxCI_C2Hoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verificar disponibilidad de GPU\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "            torch_dtype = torch.float16\n",
        "            print(\"Usando GPU para la generación de imagen.\")\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "            torch_dtype = torch.float32\n",
        "            print(\"Advertencia: GPU no disponible. Usando CPU, lo cual puede ser más lento.\")\n",
        "\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        print(\"Cargando modelo de Diffusers para text-to-image...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        # Generar imagen\n",
        "        print(\"Generando imagen...\")\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando la imagen: {str(e)}\")\n",
        "        return f\"Error generando la imagen: {str(e)}\"\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        print(\"Cargando modelo de Transformers para text-generation...\")\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        print(\"Generando texto...\")\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando el texto: {str(e)}\")\n",
        "        return f\"Error generando el texto: {str(e)}\"\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Liberar memoria antes de cargar modelos\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if task == \"text-to-image\":\n",
        "        return generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "    try:\n",
        "        print(f\"Detectando tarea: {task}\")\n",
        "        if task == \"text-to-image\":\n",
        "            return process_prompt(prompt), None\n",
        "        elif task == \"text-generation\":\n",
        "            return None, process_prompt(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando tu solicitud: {str(e)}\")\n",
        "        return None, f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "19IkL6ph2I4P",
        "outputId": "c13b512d-461b-4c48-cf1f-b73846bf7528"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7198e3a31fe9e63115.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7198e3a31fe9e63115.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# enfoque async"
      ],
      "metadata": {
        "id": "-chHg8mF2jM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "import asyncio\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "async def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verificar disponibilidad de GPU\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "            torch_dtype = torch.float16\n",
        "            print(\"Usando GPU para la generación de imagen.\")\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "            torch_dtype = torch.float32\n",
        "            print(\"Advertencia: GPU no disponible. Usando CPU, lo cual puede ser más lento.\")\n",
        "\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        print(\"Cargando modelo de Diffusers para text-to-image...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        # Generar imagen\n",
        "        print(\"Generando imagen...\")\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando la imagen: {str(e)}\")\n",
        "        return f\"Error generando la imagen: {str(e)}\"\n",
        "\n",
        "async def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        print(\"Cargando modelo de Transformers para text-generation...\")\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        print(\"Generando texto...\")\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando el texto: {str(e)}\")\n",
        "        return f\"Error generando el texto: {str(e)}\"\n",
        "\n",
        "# Paso 4: Detección de tarea\n",
        "def detect_task(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea en base al contenido del prompt.\n",
        "    \"\"\"\n",
        "    if any(keyword in prompt.lower() for keyword in [\"imagen\", \"dibujo\", \"gráfico\"]):\n",
        "        return \"text-to-image\"\n",
        "    else:\n",
        "        return \"text-generation\"  # Tarea por defecto\n",
        "\n",
        "# Paso 5: Procesar el prompt\n",
        "async def process_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Procesa el prompt y devuelve la salida según la tarea detectada.\n",
        "    \"\"\"\n",
        "    task = detect_task(prompt)\n",
        "\n",
        "    # Liberar memoria antes de cargar modelos\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    if task == \"text-to-image\":\n",
        "        return await generate_image(prompt)\n",
        "    elif task == \"text-generation\":\n",
        "        return await generate_text(prompt)\n",
        "\n",
        "# Paso 6: Interfaz con Gradio\n",
        "async def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta la tarea y devuelve el resultado en la interfaz.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Iniciando procesamiento...\")\n",
        "        text_task = asyncio.create_task(process_prompt(prompt))\n",
        "        image_task = asyncio.create_task(process_prompt(prompt))\n",
        "\n",
        "        # Esperar a que ambas tareas se completen\n",
        "        image_result = await image_task\n",
        "        text_result = await text_task\n",
        "\n",
        "        return image_result, text_result\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando tu solicitud: {str(e)}\")\n",
        "        return None, f\"Error procesando tu solicitud: {str(e)}\"\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "DihugsiW2k-I",
        "outputId": "73e7b466-efec-496e-fa60-4fa0aeb342dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f360f4b38ff460e06c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f360f4b38ff460e06c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalación de librerías necesarias\n",
        "!pip install gradio transformers diffusers accelerate safetensors -q\n",
        "\n",
        "# Paso 2: Importación y configuración\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "import gc\n",
        "\n",
        "# Modelos específicos según la tarea\n",
        "models = {\n",
        "    \"text-to-image\": \"runwayml/stable-diffusion-v1-5\",  # Texto a imagen\n",
        "    \"text-generation\": \"gpt2\",  # Generación de texto\n",
        "}\n",
        "\n",
        "# Paso 3: Funciones para cada tarea\n",
        "def generate_image(prompt):\n",
        "    \"\"\"\n",
        "    Genera una imagen basada en el texto usando Diffusers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verificar disponibilidad de GPU\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "            torch_dtype = torch.float16\n",
        "            print(\"Usando GPU para la generación de imagen.\")\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "            torch_dtype = torch.float32\n",
        "            print(\"Advertencia: GPU no disponible. Usando CPU, lo cual puede ser más lento.\")\n",
        "\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-to-image\"]\n",
        "        print(\"Cargando modelo de Diffusers para text-to-image...\")\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        # Generar imagen\n",
        "        print(\"Generando imagen...\")\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generando la imagen: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "def generate_text(prompt):\n",
        "    \"\"\"\n",
        "    Genera texto basado en un prompt usando un modelo de Transformers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cargar el modelo\n",
        "        model_id = models[\"text-generation\"]\n",
        "        print(\"Cargando modelo de Transformers para text-generation...\")\n",
        "        text_generator = pipeline(\"text-generation\", model=model_id)\n",
        "\n",
        "        # Generar texto\n",
        "        print(\"Generando texto...\")\n",
        "        output = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
        "        return output[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generando el texto: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg\n",
        "\n",
        "# Paso 4: Interfaz principal\n",
        "def main_function(prompt):\n",
        "    \"\"\"\n",
        "    Detecta las tareas y devuelve los resultados en la interfaz.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Iniciando procesamiento...\")\n",
        "        # Procesar imagen\n",
        "        print(\"Procesando imagen...\")\n",
        "        image_result = generate_image(prompt)\n",
        "\n",
        "        # Procesar texto\n",
        "        print(\"Procesando texto...\")\n",
        "        text_result = generate_text(prompt)\n",
        "\n",
        "        return image_result, text_result\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error procesando tu solicitud: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return None, error_msg\n",
        "\n",
        "# Configuración de la interfaz\n",
        "interface = gr.Interface(\n",
        "    fn=main_function,\n",
        "    inputs=gr.Textbox(label=\"Prompt\", placeholder=\"Describe lo que necesitas: texto o imagen\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Resultado (Imagen)\", type=\"pil\"),\n",
        "        gr.Textbox(label=\"Resultado (Texto)\")\n",
        "    ],\n",
        "    title=\"Chat Multi-Modo\",\n",
        "    description=\"Detecta automáticamente si necesitas texto o imágenes y usa el modelo adecuado.\"\n",
        ")\n",
        "\n",
        "# Lanzar la interfaz\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "_nbKUFSG3DfK",
        "outputId": "54b1acd8-e042-43dc-c049-f9354629681d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dcd662f8dc4bfa021b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcd662f8dc4bfa021b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}